## Architecture

참고 : https://dataflow.spring.io/docs/concepts/architecture/

<br/>



## 서버 Component 들

![](https://dataflow.spring.io/static/19e89c2894aa4586aec3336ac4e6954b/5105f/arch-overview.webp)

서버 컴포넌트는 Data Flow Server, Skipper Server 이렇게 크게 두개의 컴포넌트로 나뉘어 집니다. 그리고 Web Dashboard, Shell 을 통해서 Data Flow Server 내에서 데이터가 처리되는 현황을 볼수 있고, 데이터의 흐름을 생성하는 것 역시 가능합니다.<br/>

Data Flow Server 는 주로 데이터를 처리하고 관리하고 스트림 처리, 배치 작업의 케쥴링 하는 것과 같은 실제 데이터의 관리 제어 기능들을 수행합니다.<br/>

Skipper Server 는 주로 Stream 을 Data Flow Server 로의 배포를 담당하며, 각 Stream 에 대한 메타 정보들을 관리합니다. 배포는 블루/그린 방식으로 이뤄지며, 업그레드, 롤백, 배포 기록, Stream 에 대한 명세를 의미하는 매니페스트 파일의 history 를 저장하는 역할을 수행합니다.<br/>



### Data Flow Server

Data Flow Server 는 주로 아래와 같은 일들을 합니다.

- Parsing : DSL(Domain-Specific Language)을 기반의 스트림 및 Batch 작업 정의를 파싱합니다.
- Validating & Persisting : Stream, Task, 배치 잡 정의 들을 Validating 하거나 Persisting 합니다.
- artifact 등록 : jar, Docker Image 같은 artifact 들을 DSL 에 사용되는 이름으로 등록합니다.
- Batch Job 배포 : batch job 들을 하나 또는 하나 이상의 플랫폼에 배포합니다.
- Job 스케쥴링 : Job 의 스케쥴링을 플랫폼에 위임합니다.
- Query : Detailed Task, Batch Job 실행 history 데이터를 Query 합니다.
- 메시징 입출력 설정,배포 Properties 관리 : 메시징 입력 및 출력을 구성하는 스트림에 Configuration Properties 를 추가하고 배포 Properties (예: 초기 인스턴스 수, 메모리 요구 사항 및 데이터 분할)를 전달합니다.
- Skipper 에게 Stream 을 배포하도록 위임 : 스트림 배포를 Skipper에 위임(Delegating)합니다.
- 작업 Auditing : Stream 생성, Deployment, 배포 취소, Batch 생성, Batch Launching, Batch 등의 작업들을 Auditing 합니다. 
- Tab-완성 기능 : Stream, Batch Job 에 대해 Tab-완성 기능을 제공합니다.

<br/>



### Skipper Server

Skipper Server 를 사용하면 아래의 작업들이 가능합니다.

- 배포 : 하나 이상의 플랫폼 들에 Stream (스트림) 을 배포합니다.
- 업그레이드, 롤백 : 하나 이상의 플랫폼 들에 대해 Stream 을 Upgrade 하거나 Rollback 합니다.
- 기록 저장 : 각 Stream 의 매니페스트 파일 기록을 저장합니다.

<br/>



### Database

Data Flow Server와 Skipper Server에는 RDBMS가 설치되어 있어야 합니다. 기본적으로 서버는 내장된 H2 데이터베이스를 사용합니다. 외부 데이터베이스를 사용하도록 서버를 구성할 수 있습니다. 지원되는 데이터베이스는 H2, HSQLDB, MariaDB, Oracle, Postgresql, DB2 및 SqlServer입니다. 스키마는 각 서버가 시작될 때 자동으로 생성됩니다.

<br/>



## 애플리케이션 유형

참고 : https://dataflow.spring.io/docs/concepts/architecture/#application-types

Spring Cloud Data Flow 에서는 애플리케이션의 유형을 두가지로 분류하고, 두 가지 유형에 따라 다르게 운영이 가능해집니다.

- 장기 애플리케이션(수명이 긴 애플리케이션) : 무한에 까까운 데이터의 처리
  - 1\) 무한에 가까운 데이터가 소비되거나 생성되는 메시지 기반 애플리케이션 
  - 2\) 입력, 출력을 가지는 메시지 기반 애플리케이션. 메시징 미들웨어를 사용하지 않는 애플리케이션일 경우도 있다.
- 단기 애플리케이션(수명이 짧은 애플리케이션) : 유한한 데이터를 처리한 후 종료되는 애플리케이션
  - 1\) 코드 실행 + Data Flow 데이터베이스에 실행 상태 기록하는 작업
    - Spring Cloud Task 프레임워크를 선별적으로 사용 가능. 
    - 꼭 Java 애플리케이션일 필요는 없습니다.
    - 애플리케이션은 Data Flow 의 데이터베이스에 실행 상태를 기록해야 합니다.
  - 2\)  `1)` 의 작업을 배치 프로세싱을 지원하는 스프링 배치 프레임워크를 이용해서 수행하는 애플리케이션



장기 애플리케이션은 Spring Cloud Stream 프레임워크 기반으로 작성하고, 단기 애플리케이션은 Spring Cloud Task 프레임워크, Spring Batch 프레임워크를 기반으로 작성하는 것이 일반적인 관례입니다. Spring 을 사용하지 않는 장기 애플리케이션, 단기 애플리케이션을 작성하는 것 역시 가능합니다. 그리고 다른 언어로도 작성 가능합니다.<br/>



런타임 환경에 따라 아래의 두 가지 방식으로 애플리케이션의 패키징이 가능합니다.

- jar 파일 방식 : Spring Boot Uber Jar 라이브러리를 mvnrepository 또는 Nexus 등을 이용해서 다운로드 및 관리
- Docker 방식 : Docker 레지스트리를 통해서 이미지를 받아오고 jib, plantir, dockerfile, 등을 이용해서 패키징

<br/>



## 수명이 긴 애플리케이션 (Long-lived Applications)

- 소스, 프로세서 및 싱크가 있는 스트림
- 다중 입력 및 출력이 있는 스트림



### 소스, 프로세서 및 싱크가 있는 스트림

Spring Cloud Stream 은 아래와 같은 일반적인 메시지 교환 계약 (Exchange Contracts)를 따르는  바인딩 인터페이스를 제공합니다. 

- `Source`: 메시지를 대상으로 보내는 메시지 생성자입니다.
- `Sink`: 대상에서 메시지를 읽는 메시지 소비자입니다.
- `Processor`: 소스와 싱크의 조합입니다. 프로세서는 대상의 메시지를 소비하고 다른 대상으로 보낼 메시지를 생성합니다.



아래 예제는 쉘을 사용할 경우 수행 가능한 예제입니다.

```plain
dataflow:>app register --name http --type source --uri maven://org.springframework.cloud.stream.app:http-source-rabbit:3.2.1
Successfully registered application 'source:http'

dataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:3.2.1
Successfully registered application 'sink:log'
```

위의 예제는 소스 http, 싱크 log 를 등록하는 쉘 스크립트입니다.<br/>

Data Flow 에 등록된 http, log 를 사용하면, 아래 예제 처럼 pipe, filter 구문을 사용하는 Stream Pipeline DSL 을 이용해서 스트림 정의(Stream Definition)을 생성하는 것이 가능합니다.<br/>



### 다중 입력 및 출력이 있는 스트림





## 수명이 짧은 애플리케이션 (Short-lived Applications)

- Spring Cloud Task 애플리케이션
- Spring Batch 프레임워크

<br/>



Spring Batch 프레임워크는 Sprng Cloud Task 보다 풍부한 기능 세트를 제공하며, 대용량 데이터를 처라할 때 권장됩니다.<br/>

Spring Cloud Task 는 Spring Batch 와 통합되어 Spring Cloud Task 애플리케이션이 Spring Batch 작업을 정의한 경우 Spring Cloud Task 와 Spring Batch 실행 테이블 간의 링크가 생성됩니다.<br/>



### Composed Task

Composed Task DSL 을 이용해서 Composed Task 를 정의하는 것이 가능합니다. Composed Task DSL 에는 몇몇 기호가 있습니다.  여기에 대해서는 [reference](https://docs.spring.io/spring-cloud-dataflow/docs/2.11.2/reference/htmlsingle/#spring-cloud-dataflow-composed-tasks) 에 자세히 설명되어 있습니다. 아래는 몇몇 기호중 한가지 예입니다.

```plain
dataflow:> task create simpleComposedTask --definition "task1 && task2"
```

위의 DSL 표현식( `task1 && task2`) 은 task2 까지 모두 성공적으로 실행되어야 시작됨을 의미합니다. task1 작업 그래프는 Composed Task Runner 라는 작업 애플리케이션을 통해서 실행됩니다.<br/>



## 애플리케이션 메타 데이터



## 마이크로서비스 아키텍처 스타일

