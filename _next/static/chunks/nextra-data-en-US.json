{"/concepts/architecture":{"title":"Architecture","data":{"architecture#Architecture":"참고 : https://dataflow.spring.io/docs/concepts/architecture/","서버-component-들#서버 Component 들":"서버 컴포넌트는 Data Flow Server, Skipper Server 이렇게 크게 두개의 컴포넌트로 나뉘어 집니다. 그리고 Web Dashboard, Shell 을 통해서 Data Flow Server 내에서 데이터가 처리되는 현황을 볼수 있고, 데이터의 흐름을 생성하는 것 역시 가능합니다.Data Flow Server 는 주로 데이터를 처리하고 관리하고 스트림 처리, 배치 작업의 케쥴링 하는 것과 같은 실제 데이터의 관리 제어 기능들을 수행합니다.Skipper Server 는 주로 Stream 을 Data Flow Server 로의 배포를 담당하며, 각 Stream 에 대한 메타 정보들을 관리합니다. 배포는 블루/그린 방식으로 이뤄지며, 업그레드, 롤백, 배포 기록, Stream 에 대한 명세를 의미하는 매니페스트 파일의 history 를 저장하는 역할을 수행합니다.","data-flow-server#Data Flow Server":"Data Flow Server 는 주로 아래와 같은 일들을 합니다.\nParsing : DSL(Domain-Specific Language)을 기반의 스트림 및 Batch 작업 정의를 파싱합니다.\nValidating & Persisting : Stream, Task, 배치 잡 정의 들을 Validating 하거나 Persisting 합니다.\nartifact 등록 : jar, Docker Image 같은 artifact 들을 DSL 에 사용되는 이름으로 등록합니다.\nBatch Job 배포 : batch job 들을 하나 또는 하나 이상의 플랫폼에 배포합니다.\nJob 스케쥴링 : Job 의 스케쥴링을 플랫폼에 위임합니다.\nQuery : Detailed Task, Batch Job 실행 history 데이터를 Query 합니다.\n메시징 입출력 설정,배포 Properties 관리 : 메시징 입력 및 출력을 구성하는 스트림에 Configuration Properties 를 추가하고 배포 Properties (예: 초기 인스턴스 수, 메모리 요구 사항 및 데이터 분할)를 전달합니다.\nSkipper 에게 Stream 을 배포하도록 위임 : 스트림 배포를 Skipper에 위임(Delegating)합니다.\n작업 Auditing : Stream 생성, Deployment, 배포 취소, Batch 생성, Batch Launching, Batch 등의 작업들을 Auditing 합니다.\nTab-완성 기능 : Stream, Batch Job 에 대해 Tab-완성 기능을 제공합니다.","skipper-server#Skipper Server":"Skipper Server 를 사용하면 아래의 작업들이 가능합니다.\n배포 : 하나 이상의 플랫폼 들에 Stream (스트림) 을 배포합니다.\n업그레이드, 롤백 : 하나 이상의 플랫폼 들에 대해 Stream 을 Upgrade 하거나 Rollback 합니다.\n기록 저장 : 각 Stream 의 매니페스트 파일 기록을 저장합니다.","database#Database":"Data Flow Server와 Skipper Server에는 RDBMS가 설치되어 있어야 합니다. 기본적으로 서버는 내장된 H2 데이터베이스를 사용합니다. 외부 데이터베이스를 사용하도록 서버를 구성할 수 있습니다. 지원되는 데이터베이스는 H2, HSQLDB, MariaDB, Oracle, Postgresql, DB2 및 SqlServer입니다. 스키마는 각 서버가 시작될 때 자동으로 생성됩니다.","애플리케이션-유형#애플리케이션 유형":"참고 : https://dataflow.spring.io/docs/concepts/architecture/#application-typesSpring Cloud Data Flow 에서는 애플리케이션의 유형을 두가지로 분류하고, 두 가지 유형에 따라 다르게 운영이 가능해집니다.\n장기 애플리케이션(수명이 긴 애플리케이션) : 무한에 까까운 데이터의 처리\n1) 무한에 가까운 데이터가 소비되거나 생성되는 메시지 기반 애플리케이션\n2) 입력, 출력을 가지는 메시지 기반 애플리케이션. 메시징 미들웨어를 사용하지 않는 애플리케이션일 경우도 있다.\n단기 애플리케이션(수명이 짧은 애플리케이션) : 유한한 데이터를 처리한 후 종료되는 애플리케이션\n1) 코드 실행 + Data Flow 데이터베이스에 실행 상태 기록하는 작업\nSpring Cloud Task 프레임워크를 선별적으로 사용 가능.\n꼭 Java 애플리케이션일 필요는 없습니다.\n애플리케이션은 Data Flow 의 데이터베이스에 실행 상태를 기록해야 합니다.\n2)  1) 의 작업을 배치 프로세싱을 지원하는 스프링 배치 프레임워크를 이용해서 수행하는 애플리케이션\n장기 애플리케이션은 Spring Cloud Stream 프레임워크 기반으로 작성하고, 단기 애플리케이션은 Spring Cloud Task 프레임워크, Spring Batch 프레임워크를 기반으로 작성하는 것이 일반적인 관례입니다. Spring 을 사용하지 않는 장기 애플리케이션, 단기 애플리케이션을 작성하는 것 역시 가능합니다. 그리고 다른 언어로도 작성 가능합니다.런타임 환경에 따라 아래의 두 가지 방식으로 애플리케이션의 패키징이 가능합니다.\njar 파일 방식 : Spring Boot Uber Jar 라이브러리를 mvnrepository 또는 Nexus 등을 이용해서 다운로드 및 관리\nDocker 방식 : Docker 레지스트리를 통해서 이미지를 받아오고 jib, plantir, dockerfile, 등을 이용해서 패키징","장기-애플리케이션-long-lived-applications#장기 애플리케이션 (Long-lived Applications)":"소스, 프로세서 및 싱크가 있는 스트림\n다중 입력 및 출력이 있는 스트림","소스-프로세서-및-싱크가-있는-스트림#소스, 프로세서 및 싱크가 있는 스트림":"Spring Cloud Stream 은 아래와 같은 일반적인 메시지 교환 계약 (Exchange Contracts)를 따르는  바인딩 인터페이스를 제공합니다.\nSource: 메시지를 대상으로 보내는 메시지 생성자입니다.\nSink: 대상에서 메시지를 읽는 메시지 소비자입니다.\nProcessor: 소스와 싱크의 조합입니다. 프로세서는 대상의 메시지를 소비하고 다른 대상으로 보낼 메시지를 생성합니다.\n아래 예제는 쉘을 사용할 경우 수행 가능한 예제입니다.\ndataflow:>app register --name http --type source --uri maven://org.springframework.cloud.stream.app:http-source-rabbit:3.2.1\r\nSuccessfully registered application 'source:http'\r\n\r\ndataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:3.2.1\r\nSuccessfully registered application 'sink:log'\n위의 예제는 소스 http, 싱크 log 를 등록하는 쉘 스크립트입니다.Data Flow 에 등록된 http, log 를 사용하면, 아래 예제 처럼 pipe, filter 구문을 사용하는 Stream Pipeline DSL 을 이용해서 스트림 정의(Stream Definition)을 생성하는 것이 가능합니다.","다중-입력-및-출력이-있는-스트림#다중 입력 및 출력이 있는 스트림":"https://dataflow.spring.io/docs/concepts/architecture/#streams-with-multiple-inputs-and-outputs\n아래 내용은 의역을 했습니다. 직역을 할 경우 내용이 모호해지는 경향이 있기 때문에 의역을 통해서 요약을 했습니다. 다소 주관적인 입장으로 요약을 했기에 많은 원본 글 내용을 줄였습니다.\nSource, Sink, Processor 인터페이스의 정의를 살펴보면 단일 입력, 단일 출력을 가지고 있습니다.application properties 등에 정의된 속성들을 이용해서 입력,출력을 정의해두면 Source, Sink, Processor 와 같은 인터페이스의 정의를 이용해서 Data Flow 가 각각의 입력 대상과 출력 대상을 연결해줄 수 있게 됩니다.그런데 Source, Sink, Processor 는 단일 입력, 단일 출력입니다. 둘 이상의 입력, 출력 대상에 대해서는 바인딩이 불가능합니다. 일반적으로 메시지 처리를 하는 애플리케이션은 둘 이상의 입력 또는 출력대상이 존재하는 경우가 많습니다. Spring Cloud Stream 은 이런 경우에 대해 커스텀한 Binding 인터페이스를 사용자 정의로 직접 만들어서 사용할 수 있도록 지원합니다. 따라서 둘 이상의 입력, 출력이 있을 경우 Source, Sink, Processor 대신 커스텀한 Binding 인터페이스를 직접 정의해서 사용하는 것 역시 가능합니다.만약 다중 입력 애플리케이션을 스트림에 포함하려 할 경우 반드시 source,  sink, or processor type 대신 애플리케이션을 app 타입을 이용해서 등록해야 합니다. Stream 정의는 Stream Application DSL 이라는 문법을 사용해서 정의하는데, 이것은 || 기호를 사용해서 정의합니다. Unix, Linux 등에서 쓰이는 일반적인 단일 파이프 기호는 | 인데, Stream Application DSL 에서는 | 을 두개 씀으로써 병렬 파이프라인 이라는 것을 강조하는 기호를 사용합니다.아래는 그 예제입니다.\ndataflow:> stream create --definition \"orderGeneratorApp || baristaApp || hotDrinkDeliveryApp || coldDrinkDeliveryApp\" --name orderStream\n|| 는 병렬을 의미하는 것으로 생각하시기 바랍니다. 이 것은 응용프로그램 간의 암시적인 연결 없이 병렬을 의미합니다.\nStream Application DSL 을 이용하해서 메시징 미들웨어를 사용하지 않으면서 단일 애플리케이션을 이용해서 Stream 을 생성하는 것 역시 가능합니다.","단기-애플리케이션-short-lived-applications#단기 애플리케이션 (Short-lived Applications)":"Spring Cloud Task 애플리케이션\nSpring Batch 프레임워크\nSpring Batch 프레임워크는 Sprng Cloud Task 보다 풍부한 기능 세트를 제공하며, 대용량 데이터를 처라할 때 권장됩니다.Spring Cloud Task 는 Spring Batch 와 통합되어 Spring Cloud Task 애플리케이션이 Spring Batch 작업을 정의한 경우 Spring Cloud Task 와 Spring Batch 실행 테이블 간의 링크가 생성됩니다.","composed-task#Composed Task":"Composed Task DSL 을 이용해서 Composed Task 를 정의하는 것이 가능합니다. Composed Task DSL 에는 몇몇 기호가 있습니다.  여기에 대해서는 reference 에 자세히 설명되어 있습니다. 아래는 몇몇 기호중 한가지 예입니다.\ndataflow:> task create simpleComposedTask --definition \"task1 && task2\"\n위의 DSL 표현식( task1 && task2) 은 task2 까지 모두 성공적으로 실행되어야 시작됨을 의미합니다. task1 작업 그래프는 Composed Task Runner 라는 작업 애플리케이션을 통해서 실행됩니다.↓↓↓↓↓↓↓ 계속해서 추가 예정","애플리케이션-메타-데이터#애플리케이션 메타 데이터":"","마이크로서비스-아키텍처-스타일#마이크로서비스 아키텍처 스타일":""}},"/setup":{"title":"Setup","data":{"":"Local Machine\nDocker Compose\nDocker Compose Customization\nManual\nCloud Foundry\nKubernetes"}},"/concepts/stream-processing":{"title":"Stream Processing","data":{}},"/stream-processing":{"title":"Stream Processing","data":{}},"/":{"title":"Introduction","data":{"":"Spring Cloud Data Flow\r\nspring.io/spring-cloud-dataflow\ndataflow.spring.io"}}}