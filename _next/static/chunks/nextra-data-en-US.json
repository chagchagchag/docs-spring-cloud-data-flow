{"/concepts/stream-processing":{"title":"Stream Processing","data":{}},"/stream-processing":{"title":"Stream Processing","data":{}},"/concepts/architecture":{"title":"Architecture","data":{"architecture#Architecture":"참고 : https://dataflow.spring.io/docs/concepts/architecture/","서버-component-들#서버 Component 들":"서버 컴포넌트는 Data Flow Server, Skipper Server 이렇게 크게 두개의 컴포넌트로 나뉘어 집니다. 그리고 Web Dashboard, Shell 을 통해서 Data Flow Server 내에서 데이터가 처리되는 현황을 볼수 있고, 데이터의 흐름을 생성하는 것 역시 가능합니다.Data Flow Server 는 주로 데이터를 처리하고 관리하고 스트림 처리, 배치 작업의 케쥴링 하는 것과 같은 실제 데이터의 관리 제어 기능들을 수행합니다.Skipper Server 는 주로 Stream 을 Data Flow Server 로의 배포를 담당하며, 각 Stream 에 대한 메타 정보들을 관리합니다. 배포는 블루/그린 방식으로 이뤄지며, 업그레드, 롤백, 배포 기록, Stream 에 대한 명세를 의미하는 매니페스트 파일의 history 를 저장하는 역할을 수행합니다.","data-flow-server#Data Flow Server":"Data Flow Server 는 주로 아래와 같은 일들을 합니다.\nParsing : DSL(Domain-Specific Language)을 기반의 스트림 및 Batch 작업 정의를 파싱합니다.\nValidating & Persisting : Stream, Task, 배치 잡 정의 들을 Validating 하거나 Persisting 합니다.\nartifact 등록 : jar, Docker Image 같은 artifact 들을 DSL 에 사용되는 이름으로 등록합니다.\nBatch Job 배포 : batch job 들을 하나 또는 하나 이상의 플랫폼에 배포합니다.\nJob 스케쥴링 : Job 의 스케쥴링을 플랫폼에 위임합니다.\nQuery : Detailed Task, Batch Job 실행 history 데이터를 Query 합니다.\n메시징 입출력 설정,배포 Properties 관리 : 메시징 입력 및 출력을 구성하는 스트림에 Configuration Properties 를 추가하고 배포 Properties (예: 초기 인스턴스 수, 메모리 요구 사항 및 데이터 분할)를 전달합니다.\nSkipper 에게 Stream 을 배포하도록 위임 : 스트림 배포를 Skipper에 위임(Delegating)합니다.\n작업 Auditing : Stream 생성, Deployment, 배포 취소, Batch 생성, Batch Launching, Batch 등의 작업들을 Auditing 합니다.\nTab-완성 기능 : Stream, Batch Job 에 대해 Tab-완성 기능을 제공합니다.","skipper-server#Skipper Server":"Skipper Server 를 사용하면 아래의 작업들이 가능합니다.\n배포 : 하나 이상의 플랫폼 들에 Stream (스트림) 을 배포합니다.\n업그레이드, 롤백 : 하나 이상의 플랫폼 들에 대해 Stream 을 Upgrade 하거나 Rollback 합니다.\n기록 저장 : 각 Stream 의 매니페스트 파일 기록을 저장합니다.","database#Database":"Data Flow Server와 Skipper Server에는 RDBMS가 설치되어 있어야 합니다. 기본적으로 서버는 내장된 H2 데이터베이스를 사용합니다. 외부 데이터베이스를 사용하도록 서버를 구성할 수 있습니다. 지원되는 데이터베이스는 H2, HSQLDB, MariaDB, Oracle, Postgresql, DB2 및 SqlServer입니다. 스키마는 각 서버가 시작될 때 자동으로 생성됩니다.","애플리케이션-유형#애플리케이션 유형":"참고 : https://dataflow.spring.io/docs/concepts/architecture/#application-typesSpring Cloud Data Flow 에서는 애플리케이션의 유형을 두가지로 분류하고, 두 가지 유형에 따라 다르게 운영이 가능해집니다.\n장기 애플리케이션(수명이 긴 애플리케이션) : 무한에 까까운 데이터의 처리\n1) 무한에 가까운 데이터가 소비되거나 생성되는 메시지 기반 애플리케이션\n2) 입력, 출력을 가지는 메시지 기반 애플리케이션. 메시징 미들웨어를 사용하지 않는 애플리케이션일 경우도 있다.\n단기 애플리케이션(수명이 짧은 애플리케이션) : 유한한 데이터를 처리한 후 종료되는 애플리케이션\n1) 코드 실행 + Data Flow 데이터베이스에 실행 상태 기록하는 작업\nSpring Cloud Task 프레임워크를 선별적으로 사용 가능.\n꼭 Java 애플리케이션일 필요는 없습니다.\n애플리케이션은 Data Flow 의 데이터베이스에 실행 상태를 기록해야 합니다.\n2)  1) 의 작업을 배치 프로세싱을 지원하는 스프링 배치 프레임워크를 이용해서 수행하는 애플리케이션\n장기 애플리케이션은 Spring Cloud Stream 프레임워크 기반으로 작성하고, 단기 애플리케이션은 Spring Cloud Task 프레임워크, Spring Batch 프레임워크를 기반으로 작성하는 것이 일반적인 관례입니다. Spring 을 사용하지 않는 장기 애플리케이션, 단기 애플리케이션을 작성하는 것 역시 가능합니다. 그리고 다른 언어로도 작성 가능합니다.런타임 환경에 따라 아래의 두 가지 방식으로 애플리케이션의 패키징이 가능합니다.\njar 파일 방식 : Spring Boot Uber Jar 라이브러리를 mvnrepository 또는 Nexus 등을 이용해서 다운로드 및 관리\nDocker 방식 : Docker 레지스트리를 통해서 이미지를 받아오고 jib, plantir, dockerfile, 등을 이용해서 패키징","장기-애플리케이션-long-lived-applications#장기 애플리케이션 (Long-lived Applications)":"소스, 프로세서 및 싱크가 있는 스트림\n다중 입력 및 출력이 있는 스트림","소스-프로세서-및-싱크가-있는-스트림#소스, 프로세서 및 싱크가 있는 스트림":"Spring Cloud Stream 은 아래와 같은 일반적인 메시지 교환 계약 (Exchange Contracts)를 따르는  바인딩 인터페이스를 제공합니다.\nSource: 메시지를 대상으로 보내는 메시지 생성자입니다.\nSink: 대상에서 메시지를 읽는 메시지 소비자입니다.\nProcessor: 소스와 싱크의 조합입니다. 프로세서는 대상의 메시지를 소비하고 다른 대상으로 보낼 메시지를 생성합니다.\n아래 예제는 쉘을 사용할 경우 수행 가능한 예제입니다.\ndataflow:>app register --name http --type source --uri maven://org.springframework.cloud.stream.app:http-source-rabbit:3.2.1\r\nSuccessfully registered application 'source:http'\r\n\r\ndataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:3.2.1\r\nSuccessfully registered application 'sink:log'\n위의 예제는 소스 http, 싱크 log 를 등록하는 쉘 스크립트입니다.Data Flow 에 등록된 http, log 를 사용하면, 아래 예제 처럼 pipe, filter 구문을 사용하는 Stream Pipeline DSL 을 이용해서 스트림 정의(Stream Definition)을 생성하는 것이 가능합니다.","다중-입력-및-출력이-있는-스트림#다중 입력 및 출력이 있는 스트림":"https://dataflow.spring.io/docs/concepts/architecture/#streams-with-multiple-inputs-and-outputs\n아래 내용은 의역을 했습니다. 직역을 할 경우 내용이 모호해지는 경향이 있기 때문에 의역을 통해서 요약을 했습니다. 다소 주관적인 입장으로 요약을 했기에 많은 원본 글 내용을 줄였습니다.\nSource, Sink, Processor 인터페이스의 정의를 살펴보면 단일 입력, 단일 출력을 가지고 있습니다.application properties 등에 정의된 속성들을 이용해서 입력,출력을 정의해두면 Source, Sink, Processor 와 같은 인터페이스의 정의를 이용해서 Data Flow 가 각각의 입력 대상과 출력 대상을 연결해줄 수 있게 됩니다.그런데 Source, Sink, Processor 는 단일 입력, 단일 출력입니다. 둘 이상의 입력, 출력 대상에 대해서는 바인딩이 불가능합니다. 일반적으로 메시지 처리를 하는 애플리케이션은 둘 이상의 입력 또는 출력대상이 존재하는 경우가 많습니다. Spring Cloud Stream 은 이런 경우에 대해 커스텀한 Binding 인터페이스를 사용자 정의로 직접 만들어서 사용할 수 있도록 지원합니다. 따라서 둘 이상의 입력, 출력이 있을 경우 Source, Sink, Processor 대신 커스텀한 Binding 인터페이스를 직접 정의해서 사용하는 것 역시 가능합니다.만약 다중 입력 애플리케이션을 스트림에 포함하려 할 경우 반드시 source,  sink, or processor type 대신 애플리케이션을 app 타입을 이용해서 등록해야 합니다. Stream 정의는 Stream Application DSL 이라는 문법을 사용해서 정의하는데, 이것은 || 기호를 사용해서 정의합니다. Unix, Linux 등에서 쓰이는 일반적인 단일 파이프 기호는 | 인데, Stream Application DSL 에서는 | 을 두개 씀으로써 병렬 파이프라인 이라는 것을 강조하는 기호를 사용합니다.아래는 그 예제입니다.\ndataflow:> stream create --definition \"orderGeneratorApp || baristaApp || hotDrinkDeliveryApp || coldDrinkDeliveryApp\" --name orderStream\n|| 는 병렬을 의미하는 것으로 생각하시기 바랍니다. 이 것은 응용프로그램 간의 암시적인 연결 없이 병렬을 의미합니다.\nStream Application DSL 을 이용하해서 메시징 미들웨어를 사용하지 않으면서 단일 애플리케이션을 이용해서 Stream 을 생성하는 것 역시 가능합니다.","단기-애플리케이션-short-lived-applications#단기 애플리케이션 (Short-lived Applications)":"Spring Cloud Task 애플리케이션\nSpring Batch 프레임워크\nSpring Batch 프레임워크는 Sprng Cloud Task 보다 풍부한 기능 세트를 제공하며, 대용량 데이터를 처라할 때 권장됩니다.Spring Cloud Task 는 Spring Batch 와 통합되어 Spring Cloud Task 애플리케이션이 Spring Batch 작업을 정의한 경우 Spring Cloud Task 와 Spring Batch 실행 테이블 간의 링크가 생성됩니다.","composed-task#Composed Task":"Composed Task DSL 을 이용해서 Composed Task 를 정의하는 것이 가능합니다. Composed Task DSL 에는 몇몇 기호가 있습니다.  여기에 대해서는 reference 에 자세히 설명되어 있습니다. 아래는 몇몇 기호중 한가지 예입니다.\ndataflow:> task create simpleComposedTask --definition \"task1 && task2\"\n위의 DSL 표현식( task1 && task2) 은 task2 까지 모두 성공적으로 실행되어야 시작됨을 의미합니다. task1 작업 그래프는 Composed Task Runner 라는 작업 애플리케이션을 통해서 실행됩니다.↓↓↓↓↓↓↓ 계속해서 추가 예정","애플리케이션-메타-데이터#애플리케이션 메타 데이터":"","마이크로서비스-아키텍처-스타일#마이크로서비스 아키텍처 스타일":""}},"/concepts/advantage-when-using-scdf":{"title":"Advantage When Using Scdf","data":{"spring-cloud-data-flow-scdf-를-사용하는-것의-장점#Spring Cloud Data Flow (SCDF) 를 사용하는 것의 장점":"Spring Cloud Data Flow (SCDF) 시스템은 쿠버네티스기반으로 구축이 가능합니다. 여러 종류의 언어로 작성된 Data Source 들을 SCDF 시스템 내에서는 Stream 으로 구성하고 입력/출력으로 정의해서 내보내는 등과 같은 파이프라인 방식의 처리가 가능합니다. 즉, Data Source 들을 조합해주는 하나의 플랫폼이라고 볼 수 있습니다.Spring Cloud Data Flow (SCDF) 에서는 || 와 같은 기호를 통해 서로 다른 입력, 출력을 파이프라이닝이 가능합니다. 일반적인 리눅스 명령어에서는 파이프라이닝 시에 | 을 사용하지만, Spring Cloud Data Flow 에서는 || 기호로 파이프라이닝이 가능한데, || 은 파이프라이닝을 병렬로 처리가 가능하다는 것을 강조하는 의미를 가지고 있습니다.Spring Cloud Data Flow 기능은 만능이 아닙니다. 도입할 때 학습을 위한 시간이 꽤 많이 소요되고, 적절한 스터디를 거치고 난 후에야 적용이 가능해지게 됩니다. 다만, 도입 후에는 유지보수에 드는 여러가지 비용이 줄어든다는 점은 하나의 장점이 될수 있습니다.따라서 개발팀내에서 도입하려고 할 경우에는 운영업무와 함께 병행하면서, 조금씩 시간을 들여서 조금은 오랜 시간 안에 구축을 하기로 결정을 하고 조금씩 적용하면서 장점을 찾고, 필요한 부분들에 대해 부분적으로 도입해나가는 과정을 거듭한다면, 운영 업무와 함께 시너지를 낼 것으로 보입니다.","data-flow-서버-skipper-서버를-운용하는-것으로-인한-장점#Data Flow 서버, Skipper 서버를 운용하는 것으로 인한 장점":"Data Flow 서버, Skipper 서버로 구분되어 Data 를 처리하는 서버인 Data Flow 서버와 Data Flow Stream 을 배포하는 배포 서버인 Skipper 서버로 분리해서 용도별로 운영이 가능하며 배포가 조금 더 쉬워집니다. 배포 시에 블루/그린 배포가 가능하며 롤백이 가능하며, 배포 기록 역시 조회가 가능합니다.","data-flow-서버의-대시보드-기능#Data Flow 서버의 대시보드 기능":"데이터 스트림을 대시보드에서 생성하는 것이 가능해집니다.","특정-database-broker-종류에-종속적이지-않게-됩니다#특정 Database, Broker 종류에 종속적이지 않게 됩니다":"특정 Database, Broker 에 대한 커넥션 설정 등은 Properties 등에  필요한 설정들을 명시해두면, 구체적인 부분들은 Spring Cloud Data Flow 내에 설정된 값에 의해 바인딩이 가능해집니다. 이렇게하면, 특정 데이터 소스의 변경이 필요할 때 유연하고 빠르게 대응이 가능합니다.물론 세부적인 설정은 직접 설정해야 하지만, 인력적인 면이나, 개발 투입 시간 등의 리소스 등을 고려해보면 3달이 걸릴 일을 2주일 내에 처리 후에 세부 설정을 수정할 수 있는 경우도 있을 것 같습니다. (물론 조금 단적인 예를 들었습니다.)","단점#단점":"초기에 스터디가 필요합니다.\n시간이 필요합니다. 운영을 하면서 이 부분에 대해 인지하고 다음 버전에 어떻게 발전시켜나갈 지 고민하는 팀이라면 이 부분에 대해 천천히 준비해나간다면 충분히 대응이 가능해질 것이라고 봅니다.\n배포에 대한 내용들 역시 스터디가 어느 정도는 필요합니다.\n쿠버네티스 상으로 Data Flow, Skipper 를 배포하는 것 역시 가능한데, 실제 운영 레벨까지 사용가능하려면 이 부분들에 대해 제대로 숙지하고 어떤 부분에 문제가 있는지를 스터디를 해나가야 합니다.","그럼에도-사용하면-좋아질-수-있는-이유#그럼에도 사용하면 좋아질 수 있는 이유":"아주 간단한 데이터 처리 기능을 작성한다면 SCDF (Spring Cloud Data Flow)가 필요가 없습니다. 하지만, 데이터를 처리하는 구체적인 로직들이 많아지면, 어느 순간부터는 특정 데이터 처리를 추상화를 해야 할 경우가 생기게 됩니다. 그리고 이 추상화된 컴포넌트를 다른 기술(RabbitMQ) 등으로 바꾸게 될 수 도 있습니다. 변화가 필요한 사항이 있을 때 조금 더 빠르고 유연하게 대응할 수 있기 때문에 도입을 원한다면, 팀 내부적으로 여유를 갖고 분기 단위의 계획을 세워서 도입을 이룬다면 충분히 좋아지지 않을까 싶습니다."}},"/setup":{"title":"Setup","data":{"":"Local Machine\nDocker Compose\nDocker Compose Customization\nManual\nCloud Foundry\nKubernetes"}},"/":{"title":"Introduction","data":{"":"Spring Cloud Data Flow\r\n이미지 출처 : https://dataflow.spring.io/docs/stream-developer-guides/getting-started/stream/\nspring.io/spring-cloud-dataflow\ndataflow.spring.io\nSpring Cloud Data Flow Official Guide"}}}